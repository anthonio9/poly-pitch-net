== 5.11.2023

SIGMOID - what an huge problem it was today!

Binary crosss entropy loss is the loss used in the penn model, that I'm taking quite a lot of inspiration from, however it's not just a regular BCE, it's the BCE with LOGITS - `binary_cross_entropy_with_logits`. What is it exactly and how does it differ from binary cross entropy? Well, the difference is that BCE with logits applies the sigmoid function to the input just before Calculating the loss. The sigmoid funciton turns logits, which are the output values of the last layer in the nn model, into probabilities. 

The problem in my case was that the funciton would always output 0.69 and therefore the model wasn't capable of traning itself, 0.69 in the world of BCE means that the output no better than random. Why? Having understood what exactly is the difference between the two I decided to take a look at my model's file and THERE IT WAS, the sigmoid funciton called right after the logits output. Hopefully now that it's gone, the model will train much better!

== 19.11.2023

Investigation of PENN training input shape. FCNF0 config input data has input shape of [32, 1, 1024], FCNF0++ input data of shape [128, 1, 1024]. 

=== Pitch only

All the metric evaluation will be done using pitch only frames in the ground truth. This is how penn does it and it seems to be the better way.
The labels with no information about pitch (just silence) should have their frequency or the pitch bin randomized.

== 20.11.2023

== 24.11.2023

After the investigation of the logits and ground truth plots it seems obvious that post processing on logits trained with the gaussian blur must be broken somehow, as the plots prove that the accuracy should be much higher and actually seems to be better than the accuracy of one-hot loss approach. It definitely is much over 0%, however right now metrics estimate 0% of accuracy. Hopefully plotting the pitch will reveal what going on.

== 27.11.2023

As suspected, the accuracy of the gaussian blur approach is indeed much higher, similar to the one-hot approach. The difference between the two approaches is that the gauss-blur converges much quicker. Perhaps up to two times as quick as one-hot.

== 28.11.2023

Going 2d with more data, now it's the CQT and the time domain audio frames, pushed the accuracy over 70%. Very rarely the performance is evaluated to be below that. Overall the 2D approach seems to achieve sligthly better results than 1D (+5% or up to +10%), but it get's there much quicker! It took MonoPitchNet1D with gauss blur loss about 250 epochs to get up to 70% of RPA for the first time, where MonoPitchNet2D with gauss blur gets there about epoch nr 50 for the first time. RMSE values are much lower as well, mostly in the range 70-90 than 90-110.
